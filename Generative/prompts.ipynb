{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b54b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    df = pd.DataFrame(columns=['id', 'text'])\n",
    "\n",
    "    for file in os.listdir(file_path):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(file_path, file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "                id = file.split('.')[0]\n",
    "                df = pd.concat([df, pd.DataFrame({'id': [id], 'text': [text]})], ignore_index=True)\n",
    "\n",
    "    print(f\"Readed {len(df)} text files.\")\n",
    "    print(f\"Length unique textId: {len(df['id'].unique().tolist())}\")\n",
    "\n",
    "\n",
    "    dfAnn = pd.DataFrame(columns=['textId', 'category', 'start', 'end', 'annText'])\n",
    "\n",
    "    for file in os.listdir(file_path):\n",
    "        if file.endswith('.ann'):\n",
    "            textName = file.replace('.ann', '')\n",
    "            with open(os.path.join(file_path, file), 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    if line.startswith('T'):\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) == 3:\n",
    "                            id, category, text = parts\n",
    "                            start, end = map(int, category.split()[1:3])\n",
    "                            dfAnn = pd.concat([dfAnn, pd.DataFrame({'textId': [textName], 'category': [category.split()[0]], 'start': [start], 'end': [end], 'annText': [text]})], ignore_index=True)\n",
    "            \n",
    "    print(f\"Readed {len(dfAnn)} annotations.\")\n",
    "    print(f\"Length unique textId: {len(dfAnn['textId'].unique().tolist())}\")\n",
    "\n",
    "    #Add the whole text to the dfAnn\n",
    "    dfAnn = dfAnn.merge(df, left_on='textId', right_on='id', how='left')\n",
    "    \n",
    "    #If there are text in df that are not in dfAnn, add them\n",
    "    #dfAnn = pd.concat([dfAnn, df[~df['id'].isin(dfAnn['textId'])]], ignore_index=True)\n",
    "\n",
    "    print(f\"After merging, dfAnn has {len(dfAnn)} rows.\")\n",
    "\n",
    "    #Remove the id column from dfAnn\n",
    "    dfAnn = dfAnn.drop(columns=['id'])\n",
    "\n",
    "    #Sanity check: Check that for all the mentions the start and end are within the text length\n",
    "    for index, row in dfAnn.iterrows():\n",
    "        if row['start'] < 0 or row['end'] > len(str(row['text'])):\n",
    "            raise ValueError(f\"Start or end index out of bounds for textId {row['textId']} at index {index}.\")\n",
    "        \n",
    "    assert all(dfAnn['start'] >= 0) and all(dfAnn['end'] <= dfAnn['text'].str.len()), \"Start or end indices are out of bounds.\"\n",
    "\n",
    "    #Sanity check: check for all the mentions that the annText is in the position defined by start and end in the text\n",
    "    for index, row in dfAnn.iterrows():\n",
    "        if row['start'] < 0 or row['end'] > len(row['text']):\n",
    "            raise ValueError(f\"Start or end index out of bounds for textId {row['textId']} at index {index}.\")\n",
    "        if row['text'][row['start']:row['end']] != row['annText']:\n",
    "            #print(f\"Mismatch for textId {row['textId']} at index {index}: expected '{row['annText']}', found '{row['text'][row['start']:row['end']]}'.\")\n",
    "\n",
    "            #1. Find the occurrences in the near text (10 characters before and after the start index)\n",
    "            textToLook = row['text'][max(0, row['start'] - 10):min(len(row['text']), row['end'] + 10)]\n",
    "            #print(f\"Looking for '{row['annText']}' in textId {row['textId']} at index {index}. Text snippet: '{textToLook}'\")\n",
    "            start_index = textToLook.find(row['annText'])\n",
    "\n",
    "            #2. If the start index is not -1, and the difference between the found start index and the original start index is <= 2, adjust the start and end indices\n",
    "            if start_index != -1 and abs(start_index - (row['start'] - max(0, row['start'] - 10))) <= 2:\n",
    "                dfAnn.at[index, 'start'] = max(0, row['start'] - 10) + start_index\n",
    "                dfAnn.at[index, 'end'] = dfAnn.at[index, 'start'] + len(row['annText'])\n",
    "            \n",
    "                #print(f\"Adjusted indices for textId {row['textId']} at index {index}: new start {dfAnn.at[index, 'start']}, new end {dfAnn.at[index, 'end']}.\")\n",
    "            \n",
    "            else:\n",
    "                print(f\"Could not adjust indices for textId {row['textId']} at index {index}. Original start {row['start']}, end {row['end']}, annText '{row['annText']}' not found in text.\")\n",
    "                print(f\"Start: {row['start']}, End: {row['end']}\")\n",
    "            #print(\"------------------------------------------------------\")\n",
    "\n",
    "    assert all(dfAnn.apply(lambda row: row['text'][row['start']:row['end']] == row['annText'], axis=1)), \"AnnText does not match the text at the specified start and end indices.\"\n",
    "\n",
    "    print(f\"Length unique textId: {len(dfAnn['textId'].unique().tolist())}\")\n",
    "    print()\n",
    "    return dfAnn, df\n",
    "\n",
    "pathTrain = '../PharmacoNER/Data/train/subtrack1'\n",
    "dfAnnTrain, dfTextsTrain = read_text_file(pathTrain)\n",
    "\n",
    "pathTest = '../PharmacoNER/Data/test/subtrack1'\n",
    "pathTest = '../PharmacoNER/pharmaconer/test-set_1.1/test/subtrack1'\n",
    "dfAnnTest, dfTextsTest = read_text_file(pathTest)\n",
    "\n",
    "def getExampleAnnotated(id, df, format='html'):\n",
    "    if type(id) == int:\n",
    "        textToAnnotate = df.iloc[id]['text']\n",
    "        textId = df.iloc[id]['textId']\n",
    "        annotations = df[df['textId'] == textId]\n",
    "    elif type(id) == str:\n",
    "        textToAnnotate = df[df['textId'] == id]['text'].values[0]\n",
    "        annotations = df[df['textId'] == id]\n",
    "    else:\n",
    "        raise ValueError(\"id must be an integer or a string.\")\n",
    "    \n",
    "    if len(annotations) == 0:\n",
    "        print(f\"No annotations found for id {id}.\")\n",
    "    \n",
    "    # Create the HTML output\n",
    "    if len(annotations) == 0:\n",
    "        return textToAnnotate\n",
    "    \n",
    "    # Sort annotations by start position in descending order to avoid offset issues\n",
    "    annotations = annotations.sort_values('start', ascending=False)\n",
    "    \n",
    "    # Create a copy of the text to modify\n",
    "    html_text = textToAnnotate\n",
    "    \n",
    "    # Map entity types to CSS classes (matching your prompt format)\n",
    "    entity_class_map = {\n",
    "        'NORMALIZABLES': 'normalizables',\n",
    "        'NO_NORMALIZABLES': 'no_normalizables', \n",
    "        'PROTEINAS': 'proteinas',\n",
    "        'UNCLEAR': 'unclear'\n",
    "    }\n",
    "    \n",
    "    # Process each annotation from right to left\n",
    "    for _, annotation in annotations.iterrows():\n",
    "        start = int(annotation['start'])\n",
    "        end = int(annotation['end'])\n",
    "        entity_type = annotation['category']\n",
    "        \n",
    "        # Get the entity text\n",
    "        entity_text = html_text[start:end]\n",
    "        \n",
    "        # Get the appropriate CSS class\n",
    "        css_class = entity_class_map.get(entity_type, 'unclear')\n",
    "        \n",
    "        # Create the span tag\n",
    "        span_tag = f'<span class=\"{css_class}\">{entity_text}</span>'\n",
    "        \n",
    "        # Replace the text with the tagged version\n",
    "        html_text = html_text[:start] + span_tag + html_text[end:]\n",
    "    \n",
    "    return html_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ced0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get texts that are in dfTextsTest and not in dfAnnTest\n",
    "dfMissingTextsTrain = dfTextsTrain[~dfTextsTrain['id'].isin(dfAnnTrain['textId'])]\n",
    "dfMissingTextsTest  = dfTextsTest[~dfTextsTest['id'].isin(dfAnnTest['textId'])]\n",
    "\n",
    "print(f\"Number of texts in training set without annotations: {len(dfMissingTextsTrain)}\")\n",
    "print(f\"Number of texts in test set without annotations: {len(dfMissingTextsTest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb263a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a text that contains annotations from all the categories\n",
    "def getExampleAnnotatedAllCategories(df):\n",
    "    # Filter the DataFrame to get a text that contains annotations from all categories\n",
    "    categories = ['NORMALIZABLES', 'NO_NORMALIZABLES', 'PROTEINAS', 'UNCLEAR']\n",
    "    \n",
    "    texts = df['textId'].unique()\n",
    "\n",
    "    allTexts = []\n",
    "\n",
    "    for textId in texts:\n",
    "        annotationsCategories = df[df['textId'] == textId]['category'].unique()\n",
    "        if len(annotationsCategories) == len(categories):\n",
    "            # If the text contains annotations from all categories, return it\n",
    "            allTexts.append(textId)\n",
    "\n",
    "    return allTexts\n",
    "    \n",
    "dfAllCats = getExampleAnnotatedAllCategories(dfAnnTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe00739",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_1_TEXT = dfAnnTrain[dfAnnTrain['textId'] == dfAllCats[0]].iloc[0]['text']\n",
    "EXAMPLE_1_HTML = getExampleAnnotated(dfAllCats[0], dfAnnTrain, format='html')\n",
    "EXAMPLE_2_TEXT = dfAnnTrain[dfAnnTrain['textId'] == dfAllCats[1]].iloc[0]['text']\n",
    "EXAMPLE_2_HTML = getExampleAnnotated(dfAllCats[1], dfAnnTrain, format='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c66e2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedPromptsTrain = []\n",
    "formattedPromptsTest = []\n",
    "\n",
    "textsTrain = dfTextsTrain['id'].unique()\n",
    "textsTest  = dfTextsTest['id'].unique()\n",
    "\n",
    "#Check if some of the texts in textsTest is also in textsTrain\n",
    "for text in textsTest:\n",
    "    if text in textsTrain:\n",
    "        print(f\"Text {text} is in both train and test sets.\")\n",
    "\n",
    "#Read prompt1 from prompt1.txt\n",
    "with open('prompt2.txt', 'r', encoding='utf-8') as f:\n",
    "    prompt1 = f.read()\n",
    "\n",
    "#Delete the first line of the prompt1\n",
    "prompt1 = '\\n'.join(prompt1.split('\\n')[1:])\n",
    "\n",
    "for textId in textsTrain:\n",
    "    TARGET_TEX = dfTextsTrain[dfTextsTrain['id'] == textId]['text'].values[0]\n",
    "\n",
    "    promptFormatted = prompt1.format(\n",
    "        EXAMPLE_1_TEXT=EXAMPLE_1_TEXT,\n",
    "        EXAMPLE_1_HTML=EXAMPLE_1_HTML,\n",
    "        EXAMPLE_2_TEXT=EXAMPLE_2_TEXT,\n",
    "        EXAMPLE_2_HTML=EXAMPLE_2_HTML,\n",
    "        TEXT=TARGET_TEX\n",
    "    )\n",
    "\n",
    "    formattedPromptsTrain.append({'text': textId, 'text_to_annotate': TARGET_TEX, 'prompt': promptFormatted})\n",
    "\n",
    "for textId in textsTest:\n",
    "    TARGET_TEX = dfTextsTest[dfTextsTest['id'] == textId]['text'].values[0]\n",
    "    \n",
    "    promptFormatted = prompt1.format(\n",
    "        EXAMPLE_1_TEXT=EXAMPLE_1_TEXT,\n",
    "        EXAMPLE_1_HTML=EXAMPLE_1_HTML,\n",
    "        EXAMPLE_2_TEXT=EXAMPLE_2_TEXT,\n",
    "        EXAMPLE_2_HTML=EXAMPLE_2_HTML,\n",
    "        TEXT=TARGET_TEX\n",
    "    )\n",
    "\n",
    "    formattedPromptsTest.append({'text': textId, 'text_to_annotate': TARGET_TEX, 'prompt': promptFormatted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6b4330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a {text, type, start, end} for each annotation in the dfAnnTest\n",
    "allAnnotationsTrain = []\n",
    "allAnnotationsTest = []\n",
    "\n",
    "for textId in textsTrain:\n",
    "\n",
    "    textToAppend = dfTextsTrain[dfTextsTrain['id'] == textId].iloc[0]['text']\n",
    "    annotationsToAdd = []\n",
    "\n",
    "    for index, row in dfAnnTrain[dfAnnTrain['textId'] == textId].iterrows():\n",
    "        if textToAppend[row['start']:row['end']] != row['annText']:\n",
    "            print(f\"Mismatch for textId {row['textId']} at index {index}: expected '{row['annText']}', found '{textToAppend[row['start']:row['end']]}'.\")\n",
    "        annotationsToAdd.append({'text': row['annText'], 'type': row['category'], 'start': int(row['start']), 'end': int(row['end'])})\n",
    "\n",
    "    toAppend = {'textId': textId, 'text': textToAppend, 'annotations': annotationsToAdd}\n",
    "    \n",
    "    allAnnotationsTrain.append(toAppend)\n",
    "\n",
    "#Append first all the texts without annotations\n",
    "for textId in textsTest:\n",
    "\n",
    "    textToAppend = dfTextsTest[dfTextsTest['id'] == textId].iloc[0]['text']\n",
    "    annotationsToAdd = []\n",
    "\n",
    "    for index, row in dfAnnTest[dfAnnTest['textId'] == textId].iterrows():\n",
    "        if textToAppend[row['start']:row['end']] != row['annText']:\n",
    "            print(f\"Mismatch for textId {row['textId']} at index {index}: expected '{row['annText']}', found '{textToAppend[row['start']:row['end']]}'.\")\n",
    "        annotationsToAdd.append({'text': row['annText'], 'type': row['category'], 'start': int(row['start']), 'end': int(row['end'])})\n",
    "\n",
    "    toAppend = {'textId': textId, 'text': textToAppend, 'annotations': annotationsToAdd}\n",
    "    \n",
    "    allAnnotationsTest.append(toAppend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "590ef016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a JSONL file with the formatted prompts\n",
    "#Each json object will contain a INPUT_TEXT field with the original text to annotate\n",
    "#Also will contain a assistant field with the expected output in the format:\n",
    "#[{\"text\": \"entity text\", \"type\": \"entity type\", \"start\": start, \"end\": end}, ...]\n",
    "#The assistant field mst\n",
    "#Use 80% of the data for training and 20% for validation\n",
    "import json\n",
    "with open('instructed_prompts_train_p0.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(formattedPromptsTrain) * 80 // 100):\n",
    "        json_object = {\n",
    "            \"INPUT_TEXT\": formattedPromptsTrain[i]['text_to_annotate'],\n",
    "            \"assistant\": allAnnotationsTrain[i]['annotations']\n",
    "        }\n",
    "        f.write(json.dumps(json_object, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open('instructed_prompts_valid_p0.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(formattedPromptsTrain) * 80 // 100, len(formattedPromptsTrain)):\n",
    "        json_object = {\n",
    "            \"INPUT_TEXT\": formattedPromptsTrain[i]['text_to_annotate'],\n",
    "            \"assistant\": allAnnotationsTrain[i]['annotations']\n",
    "        }\n",
    "        f.write(json.dumps(json_object, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open('instructed_prompts_test_p0.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(formattedPromptsTest)):\n",
    "        json_object = {\n",
    "            \"INPUT_TEXT\": formattedPromptsTest[i]['text_to_annotate'],\n",
    "            \"assistant\": allAnnotationsTest[i]['annotations']\n",
    "        }\n",
    "        f.write(json.dumps(json_object, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5fc64fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 400\n",
      "Number of validation examples: 100\n",
      "Number of test examples: 250\n"
     ]
    }
   ],
   "source": [
    "#Check the lengths of the datasets\n",
    "with open('instructed_prompts_train_p0.jsonl', 'r', encoding='utf-8') as f:\n",
    "    train_lines = f.readlines()\n",
    "    print(f\"Number of training examples: {len(train_lines)}\")\n",
    "with open('instructed_prompts_valid_p0.jsonl', 'r', encoding='utf-8') as f:\n",
    "    valid_lines = f.readlines()\n",
    "    print(f\"Number of validation examples: {len(valid_lines)}\")\n",
    "with open('instructed_prompts_test_p0.jsonl', 'r', encoding='utf-8') as f:\n",
    "    test_lines = f.readlines()\n",
    "    print(f\"Number of test examples: {len(test_lines)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AEMPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
